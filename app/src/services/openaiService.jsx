// app/src/services/openaiService.jsx

import { hapticsApiStart, hapticsApiDone, hapticsApiError } from "../utils/haptics.jsx";

// === Environment & Endpoint ===
const DEV = Boolean(import.meta?.env?.DEV);
const WORKER_ORIGIN = "https://unse-openai-proxy.idcode1690.workers.dev";
const DEFAULT_MODEL = import.meta?.env?.VITE_OPENAI_MODEL || "gpt-4o-mini-2024-07-18";

const onHosted =
  typeof location !== "undefined" &&
  /github\.io|workers\.dev|vercel\.app|netlify\.app|unse\.news/i.test(location.host);

// ê°œë°œ(ë¡œì»¬)ì—ì„œë§Œ Vite í”„ë¡ì‹œ ì‚¬ìš© ê°€ëŠ¥
const CAN_USE_PROXY = DEV && !onHosted;

// ê°œë°œ(ë¡œì»¬)ì—ì„œëŠ” Vite í”„ë¡ì‹œ(/api/ai -> /chat) ì‚¬ìš©, ê·¸ ì™¸ì—ëŠ” ì›Œì»¤ ì§ì ‘ í˜¸ì¶œ
export const API_ENDPOINT = CAN_USE_PROXY ? "/api/ai" : `${WORKER_ORIGIN}/chat`;

try {
  console.info("[openaiService] endpoint:", API_ENDPOINT, "mode=", import.meta.env.MODE, "dev=", DEV);
} catch {}

// === localStorage cache ===
const CACHE_PREFIX = "ai_cache_v1:";
const inflight = new Map(); // cacheKey -> Promise<string>

const MS_MIN = 60 * 1000;
const MS_HOUR = 60 * MS_MIN;
const MS_DAY = 24 * MS_HOUR;

function nowMsKST() { return Date.now() + 9 * MS_HOUR; }
function msUntilNextKstMidnight() {
  const now = nowMsKST();
  const rem = MS_DAY - (now % MS_DAY);
  return rem > 0 ? rem : MS_MIN;
}
function msUntilNextKstSunday() {
  const now = nowMsKST();
  const d = new Date(now);
  const w = d.getUTCDay(); // 0=Sun
  const startOfToday = now - (now % MS_DAY);
  const daysToSun = (7 - w) % 7;
  const target = startOfToday + (daysToSun === 0 ? 7 : daysToSun) * MS_DAY;
  const diff = target - now;
  return diff > 0 ? diff : MS_MIN;
}
function ttlMsFromCacheKey(cacheKey) {
  const key = String(cacheKey || "").toUpperCase();
  if (key.includes("TODAY"))  return msUntilNextKstMidnight();
  if (key.includes("SAJU"))   return 365 * MS_DAY;   // ì‚¬ì£¼ëŠ” 1ë…„ ìºì‹œ
  if (key.includes("COMPAT")) return 30 * MS_DAY;
  if (key.includes("LOTTO"))  return msUntilNextKstSunday();
  return 30 * MS_DAY;
}

function lsGetRaw(k){ try { return localStorage.getItem(k); } catch { return null; } }
function lsSetRaw(k,v){ try { localStorage.setItem(k,v); } catch {} }
function lsRemove(k){ try { localStorage.removeItem(k); } catch {} }

function readCache(cacheKey) {
  if (!cacheKey) return null;
  const k = CACHE_PREFIX + cacheKey;
  const raw = lsGetRaw(k);
  if (!raw) return null;
  try {
    const { value, expireAt } = JSON.parse(raw);
    if (expireAt && Date.now() < expireAt) return String(value ?? "");
    lsRemove(k);
    return null;
  } catch { lsRemove(k); return null; }
}
function writeCache(cacheKey, value, ttlMs) {
  if (!cacheKey) return;
  const k = CACHE_PREFIX + cacheKey;
  const expireAt = Date.now() + (ttlMs ?? ttlMsFromCacheKey(cacheKey));
  lsSetRaw(k, JSON.stringify({ value: String(value ?? ""), expireAt }));
}

export function clearAICache(cacheKey) {
  if (!cacheKey) return;
  lsRemove(CACHE_PREFIX + cacheKey);
}

/**
 * callOpenAI â€” ë™ì¼ cacheKeyë©´ í•­ìƒ ë™ì¼ ê²°ê³¼ ì¬ì‚¬ìš©
 * - ì‹œì‘ ì‹œ: hapticsApiStart() 1íšŒ
 * - ì •ìƒ ì™„ë£Œ: hapticsApiDone() 1íšŒ
 * - ì˜¤ë¥˜ ì¢…ë£Œ: hapticsApiError() 1íšŒ
 */
export async function callOpenAI({
  messages,
  cacheKey,
  model = DEFAULT_MODEL,
  temperature = 0.1,   // ğŸ”’ ê±°ì˜ ê²°ì •ì 
  top_p = 1,           // ğŸ”’ ë³´ìˆ˜ì 
  max_tokens = 1800,
  seed = 777,          // ì›Œì»¤/ëª¨ë¸ì´ ì§€ì›í•˜ë©´ ê²°ì •ì„± ê°•í™”(ì§€ì› ì•ˆ í•˜ë©´ ë¬´ì‹œë¨)
} = {}) {
  // 1) cache hit
  const cached = cacheKey ? readCache(cacheKey) : null;
  if (cached != null && cached !== "") {
    try { console.debug("[openaiService] cache HIT:", cacheKey); } catch {}
    return cached;
  }
  // 2) in-flight dedupe
  if (cacheKey && inflight.has(cacheKey)) {
    try { console.debug("[openaiService] inflight dedupe:", cacheKey); } catch {}
    return inflight.get(cacheKey);
  }

  const body = { model, messages, cacheKey, temperature, top_p, max_tokens, seed };

  // í”„ë¦¬í”Œë¼ì´íŠ¸(OPTIONS) ì—†ì´ ë³´ë‚´ê¸° ìœ„í•´ text/plain ì‚¬ìš©
  const makeReq = async (endpoint) => {
    const res = await fetch(endpoint, {
      method: "POST",
      mode: "cors",
      credentials: "omit",
      cache: "no-store",
      // âœ… preflight íšŒí”¼: Simple Request (text/plain)
      headers: { "content-type": "text/plain;charset=UTF-8" },
      body: JSON.stringify(body),
      redirect: "follow",
      referrerPolicy: "no-referrer",
    });

    const text = await res.text();
    if (!res.ok) {
      let bodyJson;
      try { bodyJson = JSON.parse(text); } catch { bodyJson = { error: text }; }
      const msg = bodyJson?.error?.message || bodyJson?.error || text || "Unknown error";
      try {
        console.warn('[openaiService] FAIL', {
          endpoint,
          status: res.status,
          statusText: res.statusText,
          contentType: res.headers.get('content-type'),
          length: text.length,
          snippet: typeof text === 'string' ? text.slice(0, 160) : ''
        });
      } catch {}
      const err = new Error(`OpenAI API ì˜¤ë¥˜ (${res.status}): ${typeof msg === "string" ? msg : JSON.stringify(msg)}`);
      // ì—ëŸ¬ ê°ì²´ì— ë³´ì¡° ì •ë³´ ë¶€ì°© (ìƒíƒœ/ë³¸ë¬¸ ì¼ë¶€/ì—”ë“œí¬ì¸íŠ¸)
      err.status = res.status;
      try { err.endpoint = endpoint; } catch {}
      try { err.raw = typeof text === 'string' ? text.slice(0, 300) : ''; } catch {}
      throw err;
    }

    let data;
    try { data = JSON.parse(text); } catch { data = null; }
    const content = data?.choices?.[0]?.message?.content ?? (data ? "" : text);
    const value = String(content ?? "");

    if (cacheKey && value) writeCache(cacheKey, value, ttlMsFromCacheKey(cacheKey));
    return value;
  };

  // ğŸ”” ì‹œì‘ ì§„ë™ (ì‚¬ìš©ì ì œìŠ¤ì²˜ ì´í›„ í™˜ê²½ì—ì„œë§Œ ì‹¤ì œë¡œ ìš¸ë¦¼)
  try { hapticsApiStart(); } catch {}

  const req = (async () => {
    try {
      // 1ì°¨: í˜„ì¬ ì„¤ì •ëœ ì—”ë“œí¬ì¸íŠ¸ë¡œ
      const value = await makeReq(API_ENDPOINT);
      // ğŸ”” ì •ìƒ ì™„ë£Œ ì§„ë™
      try { hapticsApiDone(); } catch {}
      return value;
    } catch (e) {
      const msg = String(e?.message || "");
      const isNetErr = e?.name === "TypeError" || /Failed to fetch/i.test(msg);
      const is405 = /\(405\)/.test(msg) || /405 Not Allowed/i.test(msg) || /Method Not Allowed/i.test(msg);
      const startedWithProxy = API_ENDPOINT === "/api/ai";
      if (startedWithProxy) {
        // í”„ë¡ì‹œì—ì„œ ì‹¤íŒ¨ â†’ ì›Œì»¤ë¡œ í´ë°± í—ˆìš© (net/405 ëª¨ë‘)
        if (isNetErr || is405) {
          try {
            const alt = `${WORKER_ORIGIN}/chat`;
            console.warn("[openaiService] proxy failed (net/405), fallback ->", alt, e);
            const value = await makeReq(alt);
            try { hapticsApiDone(); } catch {}
            return value;
          } catch (e2) {
            try { hapticsApiError(); } catch {}
            throw e2;
          }
        }
      } else {
        // ì›Œì»¤ì—ì„œ ì‹¤íŒ¨ â†’ ê°œë°œ í™˜ê²½ì—ì„œë§Œ /api/ai í´ë°± í—ˆìš© (405ì—ì„  í´ë°± ê¸ˆì§€)
        if (CAN_USE_PROXY && isNetErr) {
          try {
            const alt = "/api/ai";
            console.warn("[openaiService] worker failed (net), fallback ->", alt, e);
            const value = await makeReq(alt);
            try { hapticsApiDone(); } catch {}
            return value;
          } catch (e2) {
            try { hapticsApiError(); } catch {}
            throw e2;
          }
        }
      }
      // ğŸ”” ì˜¤ë¥˜ ì§„ë™(ë„¤íŠ¸ì›Œí¬ ì™¸ ì—ëŸ¬ í˜¹ì€ í´ë°± ë¶ˆê°€)
      try { hapticsApiError(); } catch {}
      throw e;
    }
  })();

  if (cacheKey) inflight.set(cacheKey, req);
  try { return await req; }
  finally { if (cacheKey) inflight.delete(cacheKey); }
}
